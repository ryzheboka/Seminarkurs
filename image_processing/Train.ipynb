{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234, 288, 352, 3)\n",
      "(234, 8)\n"
     ]
    }
   ],
   "source": [
    "# Note: data isn't in the repository\n",
    "\n",
    "# loading data\n",
    "data = np.load(\"data/x_images.npy\")\n",
    "labels = np.load(\"data/y_images.npy\")\n",
    "\n",
    "# shuffling data\n",
    "indices = np.random.choice(np.arange(len(data)), size=len(data), replace=False)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# encode labels to one-hot\n",
    "num_classes = len(set((labels)))\n",
    "labels = keras.utils.to_categorical(labels, num_classes)\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pretrained model (without the top dense layers)\n",
    "\n",
    "base_model = keras.applications.DenseNet121(include_top=False, weights='imagenet', input_tensor=None, input_shape=[288,352,3], pooling=None, classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/applications/\n",
    "# add layers to the pretrained network\n",
    "x = base_model.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = keras.layers.Dense(1024, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "predictions = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 163 samples, validate on 71 samples\n",
      "Epoch 1/500\n",
      "163/163 [==============================] - 6s 36ms/step - loss: 2.4706 - acc: 0.1288 - val_loss: 5.2567 - val_acc: 0.2676\n",
      "Epoch 2/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 2.3196 - acc: 0.1288 - val_loss: 5.1016 - val_acc: 0.2254\n",
      "Epoch 3/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 2.1664 - acc: 0.1472 - val_loss: 4.8240 - val_acc: 0.2254\n",
      "Epoch 4/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 2.1837 - acc: 0.1104 - val_loss: 4.6137 - val_acc: 0.2254\n",
      "Epoch 5/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 2.1406 - acc: 0.0982 - val_loss: 4.4232 - val_acc: 0.2254\n",
      "Epoch 6/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 2.0924 - acc: 0.1840 - val_loss: 4.2877 - val_acc: 0.2254\n",
      "Epoch 7/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.9680 - acc: 0.2577 - val_loss: 4.2671 - val_acc: 0.2254\n",
      "Epoch 8/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.9735 - acc: 0.1963 - val_loss: 4.2030 - val_acc: 0.2254\n",
      "Epoch 9/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.9766 - acc: 0.2393 - val_loss: 4.1078 - val_acc: 0.2254\n",
      "Epoch 10/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.9126 - acc: 0.3067 - val_loss: 4.0555 - val_acc: 0.2254\n",
      "Epoch 11/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.8991 - acc: 0.3006 - val_loss: 4.0794 - val_acc: 0.2254\n",
      "Epoch 12/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.8329 - acc: 0.3252 - val_loss: 4.0291 - val_acc: 0.2254\n",
      "Epoch 13/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.8343 - acc: 0.3497 - val_loss: 4.0194 - val_acc: 0.2254\n",
      "Epoch 14/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.8443 - acc: 0.3006 - val_loss: 3.9733 - val_acc: 0.2254\n",
      "Epoch 15/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.7737 - acc: 0.3681 - val_loss: 4.0304 - val_acc: 0.2254\n",
      "Epoch 16/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.8624 - acc: 0.2945 - val_loss: 4.0703 - val_acc: 0.2254\n",
      "Epoch 17/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.7619 - acc: 0.3374 - val_loss: 4.0785 - val_acc: 0.2254\n",
      "Epoch 18/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.7185 - acc: 0.3497 - val_loss: 4.1204 - val_acc: 0.2254\n",
      "Epoch 19/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.7941 - acc: 0.3313 - val_loss: 4.1200 - val_acc: 0.2254\n",
      "Epoch 20/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6867 - acc: 0.3497 - val_loss: 4.2011 - val_acc: 0.2254\n",
      "Epoch 21/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.7431 - acc: 0.3681 - val_loss: 4.1635 - val_acc: 0.2254\n",
      "Epoch 22/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6793 - acc: 0.3865 - val_loss: 4.2014 - val_acc: 0.2254\n",
      "Epoch 23/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6921 - acc: 0.3436 - val_loss: 4.2437 - val_acc: 0.2254\n",
      "Epoch 24/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6221 - acc: 0.4049 - val_loss: 4.4126 - val_acc: 0.2254\n",
      "Epoch 25/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6361 - acc: 0.3558 - val_loss: 4.4494 - val_acc: 0.2254\n",
      "Epoch 26/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6112 - acc: 0.3988 - val_loss: 4.4175 - val_acc: 0.2254\n",
      "Epoch 27/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6442 - acc: 0.4110 - val_loss: 4.3773 - val_acc: 0.2254\n",
      "Epoch 28/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6476 - acc: 0.4233 - val_loss: 4.2753 - val_acc: 0.2254\n",
      "Epoch 29/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6175 - acc: 0.4110 - val_loss: 4.2394 - val_acc: 0.2254\n",
      "Epoch 30/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.6016 - acc: 0.4233 - val_loss: 4.3115 - val_acc: 0.2254\n",
      "Epoch 31/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5730 - acc: 0.4172 - val_loss: 4.3123 - val_acc: 0.2254\n",
      "Epoch 32/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5395 - acc: 0.4294 - val_loss: 4.2673 - val_acc: 0.2254\n",
      "Epoch 33/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5219 - acc: 0.4724 - val_loss: 4.3548 - val_acc: 0.2254\n",
      "Epoch 34/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5216 - acc: 0.4540 - val_loss: 4.3510 - val_acc: 0.3944\n",
      "Epoch 35/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5628 - acc: 0.3988 - val_loss: 4.3520 - val_acc: 0.3944\n",
      "Epoch 36/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5397 - acc: 0.3926 - val_loss: 4.3227 - val_acc: 0.3944\n",
      "Epoch 37/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4516 - acc: 0.4601 - val_loss: 4.3730 - val_acc: 0.3944\n",
      "Epoch 38/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5223 - acc: 0.4110 - val_loss: 4.3673 - val_acc: 0.3944\n",
      "Epoch 39/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4968 - acc: 0.4601 - val_loss: 4.3394 - val_acc: 0.3944\n",
      "Epoch 40/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5487 - acc: 0.4540 - val_loss: 4.4989 - val_acc: 0.3944\n",
      "Epoch 41/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4589 - acc: 0.4969 - val_loss: 4.4816 - val_acc: 0.3944\n",
      "Epoch 42/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4561 - acc: 0.4663 - val_loss: 4.4717 - val_acc: 0.3944\n",
      "Epoch 43/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.5017 - acc: 0.4417 - val_loss: 4.4423 - val_acc: 0.3944\n",
      "Epoch 44/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4599 - acc: 0.4172 - val_loss: 4.4267 - val_acc: 0.3944\n",
      "Epoch 45/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4809 - acc: 0.4540 - val_loss: 4.3958 - val_acc: 0.3944\n",
      "Epoch 46/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4203 - acc: 0.4969 - val_loss: 4.3987 - val_acc: 0.3944\n",
      "Epoch 47/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4469 - acc: 0.4356 - val_loss: 4.4038 - val_acc: 0.3944\n",
      "Epoch 48/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4173 - acc: 0.4417 - val_loss: 4.4743 - val_acc: 0.3944\n",
      "Epoch 49/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3642 - acc: 0.5337 - val_loss: 4.5191 - val_acc: 0.3944\n",
      "Epoch 50/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4035 - acc: 0.4847 - val_loss: 4.5556 - val_acc: 0.3944\n",
      "Epoch 51/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3962 - acc: 0.4663 - val_loss: 4.5888 - val_acc: 0.3944\n",
      "Epoch 52/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3393 - acc: 0.5276 - val_loss: 4.5916 - val_acc: 0.3944\n",
      "Epoch 53/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4090 - acc: 0.4847 - val_loss: 4.5785 - val_acc: 0.3944\n",
      "Epoch 54/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3909 - acc: 0.5276 - val_loss: 4.5584 - val_acc: 0.3944\n",
      "Epoch 55/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3512 - acc: 0.5092 - val_loss: 4.5124 - val_acc: 0.3944\n",
      "Epoch 56/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3633 - acc: 0.5337 - val_loss: 4.4866 - val_acc: 0.3944\n",
      "Epoch 57/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.4118 - acc: 0.4969 - val_loss: 4.4777 - val_acc: 0.3944\n",
      "Epoch 58/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3272 - acc: 0.5644 - val_loss: 4.5096 - val_acc: 0.3944\n",
      "Epoch 59/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3633 - acc: 0.5153 - val_loss: 4.6175 - val_acc: 0.3944\n",
      "Epoch 60/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3603 - acc: 0.5337 - val_loss: 4.6375 - val_acc: 0.3944\n",
      "Epoch 61/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3270 - acc: 0.4908 - val_loss: 4.6233 - val_acc: 0.3944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3589 - acc: 0.5460 - val_loss: 4.5830 - val_acc: 0.3944\n",
      "Epoch 63/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3689 - acc: 0.4724 - val_loss: 4.6930 - val_acc: 0.3944\n",
      "Epoch 64/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.2844 - acc: 0.5890 - val_loss: 4.8590 - val_acc: 0.3944\n",
      "Epoch 65/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.3380 - acc: 0.5031 - val_loss: 4.9153 - val_acc: 0.3944\n",
      "Epoch 66/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.2948 - acc: 0.5767 - val_loss: 4.8747 - val_acc: 0.3944\n",
      "Epoch 67/500\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 1.2554 - acc: 0.5583 - val_loss: 4.8012 - val_acc: 0.3944\n",
      "Epoch 68/500\n",
      "100/163 [=================>............] - ETA: 0s - loss: 1.2797 - acc: 0.5100"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-14b1f211766d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m               metrics=['acc'])\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "# Train the model\n",
    "history = model.fit(x=data,y=labels,batch_size=20,epochs=500,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 zero_padding2d_1\n",
      "2 conv1/conv\n",
      "3 conv1/bn\n",
      "4 conv1/relu\n",
      "5 zero_padding2d_2\n",
      "6 pool1\n",
      "7 conv2_block1_0_bn\n",
      "8 conv2_block1_0_relu\n",
      "9 conv2_block1_1_conv\n",
      "10 conv2_block1_1_bn\n",
      "11 conv2_block1_1_relu\n",
      "12 conv2_block1_2_conv\n",
      "13 conv2_block1_concat\n",
      "14 conv2_block2_0_bn\n",
      "15 conv2_block2_0_relu\n",
      "16 conv2_block2_1_conv\n",
      "17 conv2_block2_1_bn\n",
      "18 conv2_block2_1_relu\n",
      "19 conv2_block2_2_conv\n",
      "20 conv2_block2_concat\n",
      "21 conv2_block3_0_bn\n",
      "22 conv2_block3_0_relu\n",
      "23 conv2_block3_1_conv\n",
      "24 conv2_block3_1_bn\n",
      "25 conv2_block3_1_relu\n",
      "26 conv2_block3_2_conv\n",
      "27 conv2_block3_concat\n",
      "28 conv2_block4_0_bn\n",
      "29 conv2_block4_0_relu\n",
      "30 conv2_block4_1_conv\n",
      "31 conv2_block4_1_bn\n",
      "32 conv2_block4_1_relu\n",
      "33 conv2_block4_2_conv\n",
      "34 conv2_block4_concat\n",
      "35 conv2_block5_0_bn\n",
      "36 conv2_block5_0_relu\n",
      "37 conv2_block5_1_conv\n",
      "38 conv2_block5_1_bn\n",
      "39 conv2_block5_1_relu\n",
      "40 conv2_block5_2_conv\n",
      "41 conv2_block5_concat\n",
      "42 conv2_block6_0_bn\n",
      "43 conv2_block6_0_relu\n",
      "44 conv2_block6_1_conv\n",
      "45 conv2_block6_1_bn\n",
      "46 conv2_block6_1_relu\n",
      "47 conv2_block6_2_conv\n",
      "48 conv2_block6_concat\n",
      "49 pool2_bn\n",
      "50 pool2_relu\n",
      "51 pool2_conv\n",
      "52 pool2_pool\n",
      "53 conv3_block1_0_bn\n",
      "54 conv3_block1_0_relu\n",
      "55 conv3_block1_1_conv\n",
      "56 conv3_block1_1_bn\n",
      "57 conv3_block1_1_relu\n",
      "58 conv3_block1_2_conv\n",
      "59 conv3_block1_concat\n",
      "60 conv3_block2_0_bn\n",
      "61 conv3_block2_0_relu\n",
      "62 conv3_block2_1_conv\n",
      "63 conv3_block2_1_bn\n",
      "64 conv3_block2_1_relu\n",
      "65 conv3_block2_2_conv\n",
      "66 conv3_block2_concat\n",
      "67 conv3_block3_0_bn\n",
      "68 conv3_block3_0_relu\n",
      "69 conv3_block3_1_conv\n",
      "70 conv3_block3_1_bn\n",
      "71 conv3_block3_1_relu\n",
      "72 conv3_block3_2_conv\n",
      "73 conv3_block3_concat\n",
      "74 conv3_block4_0_bn\n",
      "75 conv3_block4_0_relu\n",
      "76 conv3_block4_1_conv\n",
      "77 conv3_block4_1_bn\n",
      "78 conv3_block4_1_relu\n",
      "79 conv3_block4_2_conv\n",
      "80 conv3_block4_concat\n",
      "81 conv3_block5_0_bn\n",
      "82 conv3_block5_0_relu\n",
      "83 conv3_block5_1_conv\n",
      "84 conv3_block5_1_bn\n",
      "85 conv3_block5_1_relu\n",
      "86 conv3_block5_2_conv\n",
      "87 conv3_block5_concat\n",
      "88 conv3_block6_0_bn\n",
      "89 conv3_block6_0_relu\n",
      "90 conv3_block6_1_conv\n",
      "91 conv3_block6_1_bn\n",
      "92 conv3_block6_1_relu\n",
      "93 conv3_block6_2_conv\n",
      "94 conv3_block6_concat\n",
      "95 conv3_block7_0_bn\n",
      "96 conv3_block7_0_relu\n",
      "97 conv3_block7_1_conv\n",
      "98 conv3_block7_1_bn\n",
      "99 conv3_block7_1_relu\n",
      "100 conv3_block7_2_conv\n",
      "101 conv3_block7_concat\n",
      "102 conv3_block8_0_bn\n",
      "103 conv3_block8_0_relu\n",
      "104 conv3_block8_1_conv\n",
      "105 conv3_block8_1_bn\n",
      "106 conv3_block8_1_relu\n",
      "107 conv3_block8_2_conv\n",
      "108 conv3_block8_concat\n",
      "109 conv3_block9_0_bn\n",
      "110 conv3_block9_0_relu\n",
      "111 conv3_block9_1_conv\n",
      "112 conv3_block9_1_bn\n",
      "113 conv3_block9_1_relu\n",
      "114 conv3_block9_2_conv\n",
      "115 conv3_block9_concat\n",
      "116 conv3_block10_0_bn\n",
      "117 conv3_block10_0_relu\n",
      "118 conv3_block10_1_conv\n",
      "119 conv3_block10_1_bn\n",
      "120 conv3_block10_1_relu\n",
      "121 conv3_block10_2_conv\n",
      "122 conv3_block10_concat\n",
      "123 conv3_block11_0_bn\n",
      "124 conv3_block11_0_relu\n",
      "125 conv3_block11_1_conv\n",
      "126 conv3_block11_1_bn\n",
      "127 conv3_block11_1_relu\n",
      "128 conv3_block11_2_conv\n",
      "129 conv3_block11_concat\n",
      "130 conv3_block12_0_bn\n",
      "131 conv3_block12_0_relu\n",
      "132 conv3_block12_1_conv\n",
      "133 conv3_block12_1_bn\n",
      "134 conv3_block12_1_relu\n",
      "135 conv3_block12_2_conv\n",
      "136 conv3_block12_concat\n",
      "137 pool3_bn\n",
      "138 pool3_relu\n",
      "139 pool3_conv\n",
      "140 pool3_pool\n",
      "141 conv4_block1_0_bn\n",
      "142 conv4_block1_0_relu\n",
      "143 conv4_block1_1_conv\n",
      "144 conv4_block1_1_bn\n",
      "145 conv4_block1_1_relu\n",
      "146 conv4_block1_2_conv\n",
      "147 conv4_block1_concat\n",
      "148 conv4_block2_0_bn\n",
      "149 conv4_block2_0_relu\n",
      "150 conv4_block2_1_conv\n",
      "151 conv4_block2_1_bn\n",
      "152 conv4_block2_1_relu\n",
      "153 conv4_block2_2_conv\n",
      "154 conv4_block2_concat\n",
      "155 conv4_block3_0_bn\n",
      "156 conv4_block3_0_relu\n",
      "157 conv4_block3_1_conv\n",
      "158 conv4_block3_1_bn\n",
      "159 conv4_block3_1_relu\n",
      "160 conv4_block3_2_conv\n",
      "161 conv4_block3_concat\n",
      "162 conv4_block4_0_bn\n",
      "163 conv4_block4_0_relu\n",
      "164 conv4_block4_1_conv\n",
      "165 conv4_block4_1_bn\n",
      "166 conv4_block4_1_relu\n",
      "167 conv4_block4_2_conv\n",
      "168 conv4_block4_concat\n",
      "169 conv4_block5_0_bn\n",
      "170 conv4_block5_0_relu\n",
      "171 conv4_block5_1_conv\n",
      "172 conv4_block5_1_bn\n",
      "173 conv4_block5_1_relu\n",
      "174 conv4_block5_2_conv\n",
      "175 conv4_block5_concat\n",
      "176 conv4_block6_0_bn\n",
      "177 conv4_block6_0_relu\n",
      "178 conv4_block6_1_conv\n",
      "179 conv4_block6_1_bn\n",
      "180 conv4_block6_1_relu\n",
      "181 conv4_block6_2_conv\n",
      "182 conv4_block6_concat\n",
      "183 conv4_block7_0_bn\n",
      "184 conv4_block7_0_relu\n",
      "185 conv4_block7_1_conv\n",
      "186 conv4_block7_1_bn\n",
      "187 conv4_block7_1_relu\n",
      "188 conv4_block7_2_conv\n",
      "189 conv4_block7_concat\n",
      "190 conv4_block8_0_bn\n",
      "191 conv4_block8_0_relu\n",
      "192 conv4_block8_1_conv\n",
      "193 conv4_block8_1_bn\n",
      "194 conv4_block8_1_relu\n",
      "195 conv4_block8_2_conv\n",
      "196 conv4_block8_concat\n",
      "197 conv4_block9_0_bn\n",
      "198 conv4_block9_0_relu\n",
      "199 conv4_block9_1_conv\n",
      "200 conv4_block9_1_bn\n",
      "201 conv4_block9_1_relu\n",
      "202 conv4_block9_2_conv\n",
      "203 conv4_block9_concat\n",
      "204 conv4_block10_0_bn\n",
      "205 conv4_block10_0_relu\n",
      "206 conv4_block10_1_conv\n",
      "207 conv4_block10_1_bn\n",
      "208 conv4_block10_1_relu\n",
      "209 conv4_block10_2_conv\n",
      "210 conv4_block10_concat\n",
      "211 conv4_block11_0_bn\n",
      "212 conv4_block11_0_relu\n",
      "213 conv4_block11_1_conv\n",
      "214 conv4_block11_1_bn\n",
      "215 conv4_block11_1_relu\n",
      "216 conv4_block11_2_conv\n",
      "217 conv4_block11_concat\n",
      "218 conv4_block12_0_bn\n",
      "219 conv4_block12_0_relu\n",
      "220 conv4_block12_1_conv\n",
      "221 conv4_block12_1_bn\n",
      "222 conv4_block12_1_relu\n",
      "223 conv4_block12_2_conv\n",
      "224 conv4_block12_concat\n",
      "225 conv4_block13_0_bn\n",
      "226 conv4_block13_0_relu\n",
      "227 conv4_block13_1_conv\n",
      "228 conv4_block13_1_bn\n",
      "229 conv4_block13_1_relu\n",
      "230 conv4_block13_2_conv\n",
      "231 conv4_block13_concat\n",
      "232 conv4_block14_0_bn\n",
      "233 conv4_block14_0_relu\n",
      "234 conv4_block14_1_conv\n",
      "235 conv4_block14_1_bn\n",
      "236 conv4_block14_1_relu\n",
      "237 conv4_block14_2_conv\n",
      "238 conv4_block14_concat\n",
      "239 conv4_block15_0_bn\n",
      "240 conv4_block15_0_relu\n",
      "241 conv4_block15_1_conv\n",
      "242 conv4_block15_1_bn\n",
      "243 conv4_block15_1_relu\n",
      "244 conv4_block15_2_conv\n",
      "245 conv4_block15_concat\n",
      "246 conv4_block16_0_bn\n",
      "247 conv4_block16_0_relu\n",
      "248 conv4_block16_1_conv\n",
      "249 conv4_block16_1_bn\n",
      "250 conv4_block16_1_relu\n",
      "251 conv4_block16_2_conv\n",
      "252 conv4_block16_concat\n",
      "253 conv4_block17_0_bn\n",
      "254 conv4_block17_0_relu\n",
      "255 conv4_block17_1_conv\n",
      "256 conv4_block17_1_bn\n",
      "257 conv4_block17_1_relu\n",
      "258 conv4_block17_2_conv\n",
      "259 conv4_block17_concat\n",
      "260 conv4_block18_0_bn\n",
      "261 conv4_block18_0_relu\n",
      "262 conv4_block18_1_conv\n",
      "263 conv4_block18_1_bn\n",
      "264 conv4_block18_1_relu\n",
      "265 conv4_block18_2_conv\n",
      "266 conv4_block18_concat\n",
      "267 conv4_block19_0_bn\n",
      "268 conv4_block19_0_relu\n",
      "269 conv4_block19_1_conv\n",
      "270 conv4_block19_1_bn\n",
      "271 conv4_block19_1_relu\n",
      "272 conv4_block19_2_conv\n",
      "273 conv4_block19_concat\n",
      "274 conv4_block20_0_bn\n",
      "275 conv4_block20_0_relu\n",
      "276 conv4_block20_1_conv\n",
      "277 conv4_block20_1_bn\n",
      "278 conv4_block20_1_relu\n",
      "279 conv4_block20_2_conv\n",
      "280 conv4_block20_concat\n",
      "281 conv4_block21_0_bn\n",
      "282 conv4_block21_0_relu\n",
      "283 conv4_block21_1_conv\n",
      "284 conv4_block21_1_bn\n",
      "285 conv4_block21_1_relu\n",
      "286 conv4_block21_2_conv\n",
      "287 conv4_block21_concat\n",
      "288 conv4_block22_0_bn\n",
      "289 conv4_block22_0_relu\n",
      "290 conv4_block22_1_conv\n",
      "291 conv4_block22_1_bn\n",
      "292 conv4_block22_1_relu\n",
      "293 conv4_block22_2_conv\n",
      "294 conv4_block22_concat\n",
      "295 conv4_block23_0_bn\n",
      "296 conv4_block23_0_relu\n",
      "297 conv4_block23_1_conv\n",
      "298 conv4_block23_1_bn\n",
      "299 conv4_block23_1_relu\n",
      "300 conv4_block23_2_conv\n",
      "301 conv4_block23_concat\n",
      "302 conv4_block24_0_bn\n",
      "303 conv4_block24_0_relu\n",
      "304 conv4_block24_1_conv\n",
      "305 conv4_block24_1_bn\n",
      "306 conv4_block24_1_relu\n",
      "307 conv4_block24_2_conv\n",
      "308 conv4_block24_concat\n",
      "309 pool4_bn\n",
      "310 pool4_relu\n",
      "311 pool4_conv\n",
      "312 pool4_pool\n",
      "313 conv5_block1_0_bn\n",
      "314 conv5_block1_0_relu\n",
      "315 conv5_block1_1_conv\n",
      "316 conv5_block1_1_bn\n",
      "317 conv5_block1_1_relu\n",
      "318 conv5_block1_2_conv\n",
      "319 conv5_block1_concat\n",
      "320 conv5_block2_0_bn\n",
      "321 conv5_block2_0_relu\n",
      "322 conv5_block2_1_conv\n",
      "323 conv5_block2_1_bn\n",
      "324 conv5_block2_1_relu\n",
      "325 conv5_block2_2_conv\n",
      "326 conv5_block2_concat\n",
      "327 conv5_block3_0_bn\n",
      "328 conv5_block3_0_relu\n",
      "329 conv5_block3_1_conv\n",
      "330 conv5_block3_1_bn\n",
      "331 conv5_block3_1_relu\n",
      "332 conv5_block3_2_conv\n",
      "333 conv5_block3_concat\n",
      "334 conv5_block4_0_bn\n",
      "335 conv5_block4_0_relu\n",
      "336 conv5_block4_1_conv\n",
      "337 conv5_block4_1_bn\n",
      "338 conv5_block4_1_relu\n",
      "339 conv5_block4_2_conv\n",
      "340 conv5_block4_concat\n",
      "341 conv5_block5_0_bn\n",
      "342 conv5_block5_0_relu\n",
      "343 conv5_block5_1_conv\n",
      "344 conv5_block5_1_bn\n",
      "345 conv5_block5_1_relu\n",
      "346 conv5_block5_2_conv\n",
      "347 conv5_block5_concat\n",
      "348 conv5_block6_0_bn\n",
      "349 conv5_block6_0_relu\n",
      "350 conv5_block6_1_conv\n",
      "351 conv5_block6_1_bn\n",
      "352 conv5_block6_1_relu\n",
      "353 conv5_block6_2_conv\n",
      "354 conv5_block6_concat\n",
      "355 conv5_block7_0_bn\n",
      "356 conv5_block7_0_relu\n",
      "357 conv5_block7_1_conv\n",
      "358 conv5_block7_1_bn\n",
      "359 conv5_block7_1_relu\n",
      "360 conv5_block7_2_conv\n",
      "361 conv5_block7_concat\n",
      "362 conv5_block8_0_bn\n",
      "363 conv5_block8_0_relu\n",
      "364 conv5_block8_1_conv\n",
      "365 conv5_block8_1_bn\n",
      "366 conv5_block8_1_relu\n",
      "367 conv5_block8_2_conv\n",
      "368 conv5_block8_concat\n",
      "369 conv5_block9_0_bn\n",
      "370 conv5_block9_0_relu\n",
      "371 conv5_block9_1_conv\n",
      "372 conv5_block9_1_bn\n",
      "373 conv5_block9_1_relu\n",
      "374 conv5_block9_2_conv\n",
      "375 conv5_block9_concat\n",
      "376 conv5_block10_0_bn\n",
      "377 conv5_block10_0_relu\n",
      "378 conv5_block10_1_conv\n",
      "379 conv5_block10_1_bn\n",
      "380 conv5_block10_1_relu\n",
      "381 conv5_block10_2_conv\n",
      "382 conv5_block10_concat\n",
      "383 conv5_block11_0_bn\n",
      "384 conv5_block11_0_relu\n",
      "385 conv5_block11_1_conv\n",
      "386 conv5_block11_1_bn\n",
      "387 conv5_block11_1_relu\n",
      "388 conv5_block11_2_conv\n",
      "389 conv5_block11_concat\n",
      "390 conv5_block12_0_bn\n",
      "391 conv5_block12_0_relu\n",
      "392 conv5_block12_1_conv\n",
      "393 conv5_block12_1_bn\n",
      "394 conv5_block12_1_relu\n",
      "395 conv5_block12_2_conv\n",
      "396 conv5_block12_concat\n",
      "397 conv5_block13_0_bn\n",
      "398 conv5_block13_0_relu\n",
      "399 conv5_block13_1_conv\n",
      "400 conv5_block13_1_bn\n",
      "401 conv5_block13_1_relu\n",
      "402 conv5_block13_2_conv\n",
      "403 conv5_block13_concat\n",
      "404 conv5_block14_0_bn\n",
      "405 conv5_block14_0_relu\n",
      "406 conv5_block14_1_conv\n",
      "407 conv5_block14_1_bn\n",
      "408 conv5_block14_1_relu\n",
      "409 conv5_block14_2_conv\n",
      "410 conv5_block14_concat\n",
      "411 conv5_block15_0_bn\n",
      "412 conv5_block15_0_relu\n",
      "413 conv5_block15_1_conv\n",
      "414 conv5_block15_1_bn\n",
      "415 conv5_block15_1_relu\n",
      "416 conv5_block15_2_conv\n",
      "417 conv5_block15_concat\n",
      "418 conv5_block16_0_bn\n",
      "419 conv5_block16_0_relu\n",
      "420 conv5_block16_1_conv\n",
      "421 conv5_block16_1_bn\n",
      "422 conv5_block16_1_relu\n",
      "423 conv5_block16_2_conv\n",
      "424 conv5_block16_concat\n",
      "425 bn\n",
      "426 relu\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:245]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[245:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 163 samples, validate on 71 samples\n",
      "Epoch 1/30\n",
      "163/163 [==============================] - 9s 54ms/step - loss: 1.2674 - acc: 0.5521 - val_loss: 4.9690 - val_acc: 0.3944\n",
      "Epoch 2/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2762 - acc: 0.5153 - val_loss: 4.9684 - val_acc: 0.3944\n",
      "Epoch 3/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2791 - acc: 0.5399 - val_loss: 4.9658 - val_acc: 0.3944\n",
      "Epoch 4/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.3047 - acc: 0.5460 - val_loss: 4.9637 - val_acc: 0.3944\n",
      "Epoch 5/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2712 - acc: 0.5337 - val_loss: 4.9620 - val_acc: 0.3944\n",
      "Epoch 6/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.3034 - acc: 0.5399 - val_loss: 4.9542 - val_acc: 0.3944\n",
      "Epoch 7/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2458 - acc: 0.5337 - val_loss: 4.9355 - val_acc: 0.3944\n",
      "Epoch 8/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2856 - acc: 0.5644 - val_loss: 4.9317 - val_acc: 0.3944\n",
      "Epoch 9/30\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 1.2951 - acc: 0.5399 - val_loss: 4.9516 - val_acc: 0.3944\n",
      "Epoch 10/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2304 - acc: 0.5399 - val_loss: 4.9477 - val_acc: 0.3944\n",
      "Epoch 11/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2430 - acc: 0.5460 - val_loss: 4.9444 - val_acc: 0.3944\n",
      "Epoch 12/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2822 - acc: 0.5092 - val_loss: 4.9473 - val_acc: 0.3944\n",
      "Epoch 13/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2960 - acc: 0.5706 - val_loss: 4.9425 - val_acc: 0.3944\n",
      "Epoch 14/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2668 - acc: 0.5276 - val_loss: 4.9373 - val_acc: 0.3944\n",
      "Epoch 15/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.3146 - acc: 0.4969 - val_loss: 4.9462 - val_acc: 0.3944\n",
      "Epoch 16/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2577 - acc: 0.6074 - val_loss: 4.9488 - val_acc: 0.3944\n",
      "Epoch 17/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.3188 - acc: 0.5153 - val_loss: 4.9595 - val_acc: 0.3944\n",
      "Epoch 18/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2862 - acc: 0.5706 - val_loss: 4.9475 - val_acc: 0.3944\n",
      "Epoch 19/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2359 - acc: 0.5890 - val_loss: 4.9514 - val_acc: 0.3944\n",
      "Epoch 20/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.3239 - acc: 0.5153 - val_loss: 4.9370 - val_acc: 0.3944\n",
      "Epoch 21/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.3139 - acc: 0.5092 - val_loss: 4.9284 - val_acc: 0.3944\n",
      "Epoch 22/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2943 - acc: 0.4540 - val_loss: 4.9366 - val_acc: 0.3944\n",
      "Epoch 23/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2431 - acc: 0.5767 - val_loss: 4.9462 - val_acc: 0.3944\n",
      "Epoch 24/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2264 - acc: 0.5767 - val_loss: 4.9560 - val_acc: 0.3944\n",
      "Epoch 25/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2599 - acc: 0.5092 - val_loss: 4.9590 - val_acc: 0.3944\n",
      "Epoch 26/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2689 - acc: 0.5644 - val_loss: 4.9664 - val_acc: 0.3944\n",
      "Epoch 27/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2897 - acc: 0.5031 - val_loss: 4.9554 - val_acc: 0.3944\n",
      "Epoch 28/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2966 - acc: 0.5460 - val_loss: 4.9485 - val_acc: 0.3944\n",
      "Epoch 29/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.3023 - acc: 0.5092 - val_loss: 4.9485 - val_acc: 0.3944\n",
      "Epoch 30/30\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 1.2660 - acc: 0.5460 - val_loss: 4.9590 - val_acc: 0.3944\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=1e-8),\n",
    "              metrics=['acc'])\n",
    "# Train the model\n",
    "history = model.fit(x=data,y=labels,batch_size=20,epochs=30,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = str(int(history.history[\"val_acc\"][-1]*100))\n",
    "val_loss = str(int(history.history[\"val_loss\"][-1]*10))\n",
    "filename = '_val_acc_'+val_acc+\"_val_loss_\"+val_loss\n",
    "# Save the weights\n",
    "model.save_weights('data/model/weights_'+filename+'.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('data/model/model_ar_'+filename+'.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
